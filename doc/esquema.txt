slide 1
	Introducir el trabajo o algo?...

slide 2
	Aquí vemos unas de las más famosas aplicaciones del Machine Learning. Éstas van desde los motores de búsqueda como Google, hasta la búsqueda del bosón de Higgs, pasando por un sinfín de posibilidades. Los motores de búsqueda nos ayudan a encontrar las páginas web más relevantes en apenas unos milisegundos. Los sistemas de reconocimiento de escritura pueden servir para agilizar la tramitación de envíos postales. El reconocimiento facial ayuda a la policía a encontrar delincuentes importantes. Los sistemas de recomendaciones de algunas tiendas online nos aconsejan los artículos que, por nuestras características y nuestras compras anteriores, nos pueden gustar más, evitándonos perder el tiempo buscándolos nosotros mismos. Los sistemas de detección de correo no deseado, mediante el procesamiento del lenguaje, nos ayudan a evitar leer emails que no son de nuestro interés, y la detección de fraudes en tarjetas de crédito nos aseguran que nuestro dinero está seguro. Tal vez en el futuro, con el desarrollo de leyes que permitan su uso, los coches autoconducidos nos ayudarán a tener ciudades con menos atascos y contaminación, más seguras y con menos accidentes. Para acabar, la clasificación de secuencias de ADN permite a los investigadores entender mejor el funcionamiento de nuestra genética y de algunas enfermedades como el cáncer, aumentando nuestra calidad de vida.Estas puede que sean las aplicaciones más famosas y relevantes del Machine Learning, pero hay muchas más, si tratásemos de incluirlas todas en una presentación, ocuparían muchísimas más páginas y estaríamos días hablando de ellas.

slide 3
	Pero, ¿cómo ayuda el Machine Learning a detectar el bosón de Higgs? En el LHC se producen cientos de millones de colisiones protón-protón por segundo. Los cientos de partículas que se producen en cada cruce de paquetes de partículas, lo que llamamos un evento, son detectadas, producioendo vectores de unos cientos de miles de variables, de los que podemos estimar medidas como la dirección de cada partícula, de qué tipo es, o su energía. Todos estos datos se recogen finalmente en vectores de decenas de variables. Algunas de estas variables se utilizan en tiempo real para descartar la mayoría de eventos sin interés, llamados background, y los aproximadamente 400 eventos por segundo restantes se almacenan en discos duros, produciendo petabytes de datos al año. Estos eventos aún contienen una mayoría de eventos conocidos, también background, producidos por el decaimiento de partículas exóticas, pero ya descubiertas anteriormente. El objetivo es encontrar una región en el espacio de variables en la que haya significativamente más eventos de los que podemos explicar con las partículas conocidas. A esos eventos los llamaremos señal (o signal en inglés), y es donde estará la partícula que desconocemos.

slide 4
	Ahora vamos a ver cómo logramos identificar la región en la que estará el Higgs. Para empezar, vamos a diferenciar dos tipos de problemas, uno conocido, el de regresión, y otro tal vez menos común, como es el de clasificación. En el primero, trataremos de predecir el valor de una variable en un continuo de valores posibles, mientras que en el segundo tendremos sólo un conjunto discreto de valores de la variable a predecir. A cada valor de este conjunto lo llamaremos clase, y podemos tener solo dos clases (clasificación binaria) o varias (multiclase). El problema del Higgs es de clasificación binaria, teniendo las clases background y señal.

slide 5
	En Machine Learning tenemos dos tipos de datos con los que entrenar nuestros algoritmos. En el primero, de cada muestra sabemos a qué clase pertenece, se llama aprendizaje supervisado. En el segundo, sin embargo, no sabemos las clases de las muestras, por lo que habrá que buscar grupos de datos (clusters en inglés) que sigan los mismos patrones, se llama aprendizaje no supervisado. Los algoritmos empleados en cada tipo de aprendizaje no son los mismos, nosotros trataremos con aprendizaje supervisado. Para entrenar los algoritmos tendremos tres conjuntos, el de aprendizaje, que sirve para entrenar el algoritmo, el de validación cruzada, que sirve para fijar los parámetros del mismo, y el de test, para ver cómo generaliza.

slide 6
	El objetivo de todos los algoritmos de clasificación es el de conseguir una superficie de decisión, separando el espacio de variables en tantas regiones como clases a las que pertenezcan los datos. Estas regiones no tienen que ser necesariamente conexas. Cada algoritmo elegirá la superficie de decisión de una forma distinta, teniendo por ejemplo la forma característica de los Random Forest. Gracias a estas superficies de decisión, el algoritmo podrá clasificar entradas nuevas de datos según los valores de sus variables, asignándoles la clase que corresponda.

slide 7
	Los dos grandes problemas a los que nos enfrentamos al emplear el Machine Learning son el sesgo y la varianza. Tendremos alto sesgo cuando nuestra hipótesis sea demasiado sencilla o no tengamos suficientes variables de las que extraer información. El algoritmo no clasifica bien ni el conjunto de aprendizaje ni el de test, estamos cometiendo un underfitting (infraajuste). Se puede solucionar tomando más variables y más datos, o haciendo una hipótesis más compleja. Por otra parte, tendremos alta varianza cuando nuestra hipótesis dependa demasiado de variaciones concretas de los datos de aprendizaje. Nuestra hipótesis es demasiado compleja y no tenemos suficientes datos. clasificará muy bien los datos del conjunto de aprendizaje, pero generalizará mal para los del conjunto de test. Estamos cometiendo un overfitting (sobreajuste). Se puede solucionar tomando más datos, simplificando la hipótesis o penalizando los parámetros de ésta en la función de error a minimizar.

slide 8
	Para ver si nuestro algoritmo está generalizando bien los datos, disponemos de varias métricas. Accuracy (exactitud) nos da el cociente entre los casos bien clasificados y el total de casos. Recall (sensibilidad) nos da el cociente entre los clasificados correctamente como positivos y todos los positivos reales. Specificity (especificidad) nos da el de los casos clasificados correctamente como negativos y todos los clasificados como tales. Precision (precisión) nos da el de los casos correctamente clasificados como positivos y todos los clasificados positivos. Por último, el valor F1 es la media armónica de la sensibilidad y la precisión. ¿DEBERÍA HABLAR DE LAS CLASES SESGADAS?

slide 9
	Pasamos a explicar los algoritmos utilizados. El más sencillo de todos seguramente sea el K-Nearest Neighbors, en el que a cada punto se le asigna la clase de las K muestras del conjunto de entrenamiento más cercanas. El problema es que si no ajustamos bien el parámetro K podemos cometer fácilmente un sobreajuste, como podemos ver en esta gráfica, en la que se emplea un vecino. Subiendo el número de vecinos conseguimos generalizar mejor, aunque no clasifiquemos bien los puntos aislados.

slide 10
	El algoritmo de la regresión logística es otro algoritmo de clasificación. Se le asigna a cada variable un peso, y se suman las variables multiplicadas por su peso. La probabilidad de que una entrada de datos sea de la clase 1 es el resultado de aplicar la función logística sobre la suma anterior. El error cometido para una entrada de datos es proporcional a menos el logaritmo de la probabilidad de que fuese de la clase a la que pertenece, así, si el algoritmo da una probabilidad de uno, no cometemos error, pero si su probabilidad es muy baja, el error cometido se dispara.

slide 11
	El algoritmo de Bayes Ingenuo utiliza el teorema de Bayes, que nos dice que, dado un vector de variables x, la probabilidad de que esa entrada de datos sea de la clase C_l es la probabilidad de que se dé esa clase por la probabilidad de que dada esa clase se dé ese vector de variables, entre la probabilidad de que se dé ese vector de variables. Como dado un vector, la probabilidad de que éste se dé es siempre la misma, sólo consideraremos el numerador de la ecuación. Se llama Ingenuo porque suponemos que todas las variables son independientes entre sí, quedando esta ecuación. El algoritmo asignará a cada entrada la clase más probable, utilizando tanto el teorema de Bayes como la distribución de probabilidad de cada variable.


slide 12
	Pasamos a los Support Vector Machines. De estas tres líneas de decisión, la que esperamos que sea mejor es la B. Esto es porque, si los datos nuevos que tomamos siguen la misma distribución que los datos que ya tenemos, las líneas A y C pueden empezar a clasificar mal algunos datos. Las SVM tratan de minimizar el error de generalización, el error que podamos cometer al tomar datos nuevos, gracias al método del high margin.

slide 13
	Los vectores de soporte son los puntos más cercanos a la curva de decisión, y el objetivo del método de high margin consiste en buscar la mayor distancia entre éstos y la línea de decisión. A veces esto no será posible, debido a la distribución de los puntos sobre el espacio de las variables. En estos casos emplearemos el método de soft margin, introduciendo una variable de holgura para cada punto del conjunto de aprendizaje, de manera que permitimos que se clasifiquen mal algunos puntos con tal de conseguir la mejor línea de decisión. Estas variables de holgura se penalizarán en la función de error a minimizar, empleando el parámetro de smoothing.

slide 14
	Los árboles de decisión son unos algoritmos bastante sencillos, que van dividiendo cada región del espacio de las variables en dos, tratando de minimizar una medida del desorden de los datos, como puede ser la entropía de Shannon. Los árboles de decisión son muy propensos al sobreajuste, dejando regiones aisladas con unos pocos datos, como podemos ver aquí.

slide 15
	Los Random Forest son uno de los llamados Ensemble Methods, en los que se ponen en conjunto varios algoritmos, en este caso árboles de decisión, para asignar a cada entrada de datos la clase más votada por éstos. Así solucionamos problemas como el sobreajuste de los árboles. Consiste en hacer crecer varios árboles, cada uno con un subconjunto de los datos de entrenamiento y un subconjunto de las variables. La forma típica de la superficie de decisión es la que vemos aquí.

slide 16
	Una vez hemos explicado ya todos los algoritmos, volvemos al problema de la búsqueda del Higgs. Aquí vemos las métricas conseguidas por los algoritmos entrenados con esta base de datos. Podemos ver que el Naïve Bayes gaussiano no está consiguiendo clasificar muy bien los datos, mientras que la SVM y el Random Forest lo hacen bastante bien. Están consiguiendo una exactitud de más del 80%, con valores F1 en torno al 87%. Podríamos pensar que el Random Forest es el mejor de los algoritmos para esta base de datos, pero... CAMBIO DE DIAPOSITIVA

slide 17
	...si miramos su curva de aprendizaje, vemos claramente que está ajustando demasiado bien los puntos del conjunto de entrenamiento, estamos sobreajustando los datos, así que no nos podemos fiar de su capacidad de generalización. Vemos también cómo el Naïve Bayes está infraajustando los datos, no consigue buenas métricas para  ninguno de los dos conjuntos. El SVM, sin embargo, no comete ni overfitting ni underfitting, consigue buenas métricas para ambos conjuntos, así que lo podremos utilizar para que nos ayude a encontrar el Higgs.

slide 18
	LO DEJO ASÍ? PONGO COSAS COMO QUE TARDÉ 3 DÍAS EN HACER LAS CURVAS DE APRENDIZAJE DEL HIGGS? PONGO RESULTADOS DEL HIGGS, COMO, POR EJEMPLO, VALORES DE LAS MÉTRICAS PARA CADA ALGORITMO? (El título de la diapositiva lo tengo que cambiar, no sé si poner algo así como "Especificaciones", "Resultados" o algo así...)

